{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(sampling, image_num, distance, features):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        image_num (int): image number to load the corresponding csv file\n",
    "        distance (float): filter out distance\n",
    "        features (list): list of features \n",
    "    \"\"\"\n",
    "    data = pd.read_csv(f'../data/processed/{sampling}/max/STD/Image_{image_num}_max_STD.csv')\n",
    "    entropy = pd.read_csv(f'../data/processed/{sampling}/max/Entropy/Image_{image_num}_max_Entropy.csv')\n",
    "    depth = pd.read_csv(f'../data/processed/{sampling}/max/STD/Image_{image_num}_max_STD.csv')\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    new_df['max_STD'] = data['max_STD']\n",
    "    new_df['max_Entropy'] = entropy['max_Entropy']\n",
    "    new_df['depth_value'] = depth['depth_value'] \n",
    "    new_df['x'] = depth['x'] \n",
    "    new_df['y'] = depth['y'] \n",
    "    \n",
    "    new_df[\"label\"] = np.where(new_df['depth_value'] <= distance, 1, 0)\n",
    "\n",
    "    max_labels = new_df[new_df.label == 0]\n",
    "    min_labels = new_df[new_df.label == 1]\n",
    "\n",
    "    max_down = resample(max_labels,\n",
    "                        replace=False,\n",
    "                        n_samples=len(min_labels),     # match minority\n",
    "                        random_state=42)\n",
    "\n",
    "    df_balanced = pd.concat([max_down, min_labels])\n",
    "    \n",
    "    return df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"out\"\n",
    "SAMPLING = \"TopDown\"\n",
    "# IMAGE_NUM = 5\n",
    "\n",
    "# TopDown (Done): \n",
    "#          3CM , 5CM, 0.009CM\n",
    "#          STD, Entropy, (Std, Entropy)\n",
    "\n",
    "# Circular : \n",
    "#          3CM , 5CM, 0.009 CM\n",
    "#          STD, Entropy, (Std, Entropy)\n",
    "\n",
    "# FEATURES = ['max_STD']\n",
    "# FEATURES = ['max_Entropy']\n",
    "FEATURES = ['max_STD', 'max_Entropy']\n",
    "\n",
    "DISTANCE = 0.03\n",
    "\n",
    "feats = \"_\".join([i.split(\"_\")[-1].upper() for i in FEATURES])\n",
    "EXP_NAME = feats+\"_\"+str(DISTANCE)+\"_CM\"\n",
    "\n",
    "try:\n",
    "    if not os.path.isdir(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME)):\n",
    "        os.makedirs(os.path.join(\"../\",DIR, SAMPLING, EXP_NAME))\n",
    "except:\n",
    "    print(\"Folder Exist\")\n",
    "\n",
    "accuracy_curve = []\n",
    "\n",
    "for img_num in range(1, 101):\n",
    "    # FOLDER_NAME = \"IMAGE_NUM_\"+str(img_num)\n",
    "    if img_num == 100:\n",
    "        DISTANCE = 0.029\n",
    "    data = data_loader(SAMPLING, img_num, DISTANCE, FEATURES)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, data, test_size=0.2, random_state=42)\n",
    "\n",
    "    data_len = len(X_test)\n",
    "\n",
    "    image = np.zeros((1024, 1024))\n",
    "    for i in range(data_len):\n",
    "        X = int(X_test.iloc[i]['x'])\n",
    "        Y = int(X_test.iloc[i]['y'])\n",
    "        \n",
    "        image[X, Y] = int(X_test.iloc[i]['label'])\n",
    "        \n",
    "\n",
    "    # try:\n",
    "    #     if not os.path.isdir(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME, FOLDER_NAME)):\n",
    "    #         os.makedirs(os.path.join(\"../\",DIR, SAMPLING, EXP_NAME, FOLDER_NAME))\n",
    "    # except:\n",
    "    #     print(\"Folder Exist\")\n",
    "    \n",
    "    cv2.imwrite(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME, f\"{img_num}.png\"), image * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Test the model using test dataset from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"out\"\n",
    "MODEL_DIR = \"logs\"\n",
    "SAMPLING = \"TopDown\"\n",
    "# IMAGE_NUM = 5\n",
    "\n",
    "# TopDown (Done): \n",
    "#          3CM , 5CM, 0.009CM\n",
    "#          STD, Entropy, (Std, Entropy)\n",
    "\n",
    "# Circular : \n",
    "#          3CM , 5CM, 0.009 CM\n",
    "#          STD, Entropy, (Std, Entropy)\n",
    "\n",
    "# FEATURES = ['max_STD']\n",
    "FEATURES = ['max_Entropy']\n",
    "# FEATURES = ['max_STD', 'max_Entropy']\n",
    "\n",
    "DISTANCE = 0.03\n",
    "\n",
    "feats = \"_\".join([i.split(\"_\")[-1].upper() for i in FEATURES])\n",
    "EXP_NAME = feats+\"_\"+str(DISTANCE)+\"_CM\"\n",
    "\n",
    "try:\n",
    "    if not os.path.isdir(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME)):\n",
    "        os.makedirs(os.path.join(\"../\",DIR, SAMPLING, EXP_NAME))\n",
    "except:\n",
    "    print(\"Folder Exist\")\n",
    "\n",
    "accuracy_curve = []\n",
    "\n",
    "for img_num in range(1, 101):\n",
    "    FOLDER_NAME = \"IMAGE_NUM_\"+str(img_num)\n",
    "    if img_num == 100:\n",
    "        DISTANCE = 0.029\n",
    "    data = data_loader(SAMPLING, img_num, DISTANCE, FEATURES)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, data, test_size=0.2, random_state=42)\n",
    "    model = joblib.load(os.path.join(\"../\", MODEL_DIR, SAMPLING, EXP_NAME, FOLDER_NAME, \"model.pkl\")) \n",
    "\n",
    "  \n",
    "    training_feats = np.array(X_train[FEATURES]) #['max_STD', 'max_Entropy']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(training_feats)\n",
    "    # X_test_scaled = scaler.transform(X_test)\n",
    "    # print(X_test_scaled)\n",
    "    # sdf\n",
    "\n",
    "\n",
    "    data_len = len(X_test)\n",
    "    image = np.zeros((1024, 1024))\n",
    "    for i in range(data_len):\n",
    "        X = int(X_test.iloc[i]['x'])\n",
    "        Y = int(X_test.iloc[i]['y'])\n",
    "        \n",
    "        # feats = np.array([[X_test.iloc[i]['max_STD'], X_test.iloc[i]['max_Entropy']]])\n",
    "        feats = np.array([[X_test.iloc[i]['max_STD']]])\n",
    "        image[X, Y] = int(model.predict(scaler.transform(feats)).item())\n",
    "        \n",
    "\n",
    "    # try:\n",
    "    #     if not os.path.isdir(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME, FOLDER_NAME)):\n",
    "    #         os.makedirs(os.path.join(\"../\",DIR, SAMPLING, EXP_NAME, FOLDER_NAME))\n",
    "    # except:\n",
    "    #     print(\"Folder Exist\")\n",
    "    \n",
    "    cv2.imwrite(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME, f\"{img_num}.png\"), image * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792016603124342"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_avg = np.load('..\\logs\\TopDown\\STD_ENTROPY_0.03_CM\\STD_ENTROPY_0.03_CM_accuracy_curve.npy')\n",
    "np.mean(accuracy_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "from PIL import Image\n",
    "numbers = re.compile(r'(\\d+)')\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"data/damage_crop\"\n",
    "MODEL_DIR = \"logs\"\n",
    "SAMPLING = \"TopDown\"\n",
    "FEATURES = ['max_STD', 'max_Entropy']\n",
    "DISTANCE = 0.03\n",
    "\n",
    "IMAGES = sorted(glob.glob(os.path.join('../data/damage_crop/STD', '*.tiff')), key=numericalSort)\n",
    "\n",
    "feats = \"_\".join([i.split(\"_\")[-1].upper() for i in FEATURES])\n",
    "EXP_NAME = feats+\"_\"+str(DISTANCE)+\"_CM\"\n",
    "\n",
    "try:\n",
    "    if not os.path.isdir(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME)):\n",
    "        os.makedirs(os.path.join(\"../\",DIR, SAMPLING, EXP_NAME))\n",
    "except:\n",
    "    print(\"Folder Exist\")\n",
    "\n",
    "accuracy_curve = []\n",
    "\n",
    "for img_num, path in enumerate(IMAGES):\n",
    "    FOLDER_NAME = \"IMAGE_NUM_\"+str(img_num + 1)\n",
    "    if img_num+1 == 100:\n",
    "        DISTANCE = 0.029\n",
    "    data = data_loader(SAMPLING, img_num + 1, DISTANCE, FEATURES)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, data, test_size=0.2, random_state=42)\n",
    "    model = joblib.load(os.path.join(\"../\", MODEL_DIR, SAMPLING, EXP_NAME, FOLDER_NAME, \"model.pkl\")) \n",
    "\n",
    "  \n",
    "    training_feats = np.array(X_train[['max_STD', 'max_Entropy']])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(training_feats)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_len = len(X_test)\n",
    "    results = np.zeros((1024, 1024))\n",
    "\n",
    "    std_img = Image.open(path)\n",
    "    path = path.replace('STD', 'entropy').replace('max_variance', 'max_entropy')\n",
    "    entropy_img = Image.open(path)\n",
    "    \n",
    "    std_img = np.array(std_img)\n",
    "    entropy_img = np.array(entropy_img)\n",
    "    \n",
    "    H, W = std_img.shape\n",
    "    flat_std = std_img.flatten()\n",
    "    flat_entropy = entropy_img.flatten()\n",
    "\n",
    "    # Each pixel gets a feature vector: [std, std]\n",
    "    feats = np.stack([flat_std, flat_entropy], axis=1)  # shape: (H*W, 2)\n",
    "\n",
    "    # Transform and predict in batch\n",
    "    scaled_feats = scaler.transform(feats)\n",
    "    preds = model.predict(scaled_feats)\n",
    "\n",
    "    # Reshape predictions back to image shape\n",
    "    results = preds.reshape(H, W).astype(int)\n",
    "    '''\n",
    "    for X in range(1024):\n",
    "        for Y in range(1024):\n",
    "            feats = np.array([[std_img[X, Y], std_img[X, Y]]])\n",
    "            results[X, Y] = int(model.predict(scaler.transform(feats)).item())\n",
    "    '''\n",
    "    cv2.imwrite(os.path.join(\"../\", DIR, SAMPLING, EXP_NAME, f\"{img_num+1}.png\"), results * 255)\n",
    "    #print(img_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
